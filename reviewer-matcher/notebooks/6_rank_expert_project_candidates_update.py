# -*- coding: utf-8 -*-
"""6-rank_expert-project_candidates-UPDATE.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1l-cjwh08tPNRpI4zi-1M1kQsw2BJUhoD
"""

# TODO: Modify files before saving in the notebook that generates the data and saves the Pickle files so this is not necessary.
!pip install -q abbreviations

import pandas as pd
import abbreviations
import re
import numpy as np

from google.auth import default
from google.colab import auth
from google.colab import drive

from tqdm import tqdm

from google.colab import drive
drive.mount('/content/drive')

### Input/output paths.

# Specific call folder (used to retrieve the configuration, URLs, etc).
CALL_NAME = '2021-Salut Mental'

# Bath path for all the sample data.
BASE_PATH = '/content/drive/MyDrive/1_Current_projects_SIRIS/2024AQUAS-ReviewerMatcher'

# Code path.
CODE_PATH = f'{BASE_PATH}/Implementation/Notebooks'

# Data path.
DATA_PATH = f'{BASE_PATH}/Implementation/Data'

# Input files.
FILE_PATH_PROJECTS = f'{DATA_PATH}/{CALL_NAME}/projects.pkl'
FILE_PATH_EXPERTS = f'{DATA_PATH}/{CALL_NAME}/experts.pkl'

FILE_PATH_EXPERT_PROJECT_CONTENT_SIMILARITY_SCORES = f'{DATA_PATH}/{CALL_NAME}/scores/expert_projects_content_similarity_scores.pkl'
FILE_PATH_EXPERT_PROJECT_MESH_SCORES = f'{DATA_PATH}/{CALL_NAME}/scores/expert_projects_mesh_scores.pkl'
FILE_PATH_EXPERT_PROJECT_JACCARD_SIMILARITY = f'{DATA_PATH}/{CALL_NAME}/scores/expert_project_jaccard_similarity_scores.pkl'

# Output file
FILE_PATH_COMBINED_SCORES = f'{DATA_PATH}/{CALL_NAME}/scores/expert_projects_combined_scores.tsv'

# For evaluation
FILE_MANUAL_ASSIGNMENTS = FILE_PATH_COMBINED_SCORES = f'{DATA_PATH}/{CALL_NAME}/manual_assignments.tsv'

### Settings

SEPARATOR_VALUES = '|'

NUM_PUBS_TOP_PERC_SENIORITY = 30
NUM_CITATIONS_TOP_PERC_SENIORITY = 30

### Settings for weights

WEIGHT_RESEARCH_AREAS_JACCARD = 0.5
WEIGHT_RESEARCH_APPROACHES_JACCARD = 0.5

WEIGHT_TOPICS = 0.5
WEIGHT_METHODS = 0.5

### Test size if we want to process a subset for testing. Set as 0 to ignore.

TEST_SIZE_PROJECTS = 0
TEST_SIZE_EXPERTS = 0

# Load data.

df_projects = pd.read_pickle(FILE_PATH_PROJECTS).fillna('')
df_experts = pd.read_pickle(FILE_PATH_EXPERTS).fillna('')

if TEST_SIZE_PROJECTS:
  df_projects = df_projects.head(TEST_SIZE_PROJECTS)

if TEST_SIZE_EXPERTS:
  df_experts = df_experts.head(TEST_SIZE_EXPERTS)

### Functions

# Function to extract the first valid numeric value from a text
def extract_number(text):
#--------------------------------
    if type(text) == int:
      return text
    # Check for empty or NaN values
    if pd.isna(text) or text.strip() == '':
        return 0
    # Check if the entire text is a URL, if so return 0
    if re.match(r'^(https?://)', text.strip()):
        return 0
    # Extract all numbers, including those with commas
    numbers = re.findall(r'\b\d{1,10}(?:,\d{3})*|\d+\b', text)
    if numbers:
        # Clean commas and convert all numbers to integers
        cleaned_numbers = [int(num.replace(',', '')) for num in numbers]
        # Return the largest number found
        return max(cleaned_numbers)
    return 0

# Condition for being senior: experience in both columns, and publications and citations in top third
def determine_seniority(row):
#--------------------------------
    if row['EXPERIENCE_REVIEWER'].lower() == 'yes' and \
       row['EXPERIENCE_PANEL'].lower() == 'yes' and \
       row['NUMBER_PUBLICATIONS'] >= pub_threshold and \
       row['NUMBER_CITATIONS'] >= citations_threshold:
       return 'yes'
    return 'no'

### Functions for step 1: Implement the different strategies to test.

# Function to normalize and calculate weighted sum
def normalize_and_weight(series, weight):
#--------------------------------
    return (series - series.min()) / (series.max() - series.min()) * weight

### Strategy 1: Score based on Jaccard similarity
def calculate_score_jaccard(df):
#--------------------------------
    # Calculate weighted scores for research areas and research approaches
    #research_areas_score = normalize_and_weight(df['Research_Areas_Jaccard_Similarity'], WEIGHT_RESEARCH_AREAS_JACCARD)
    #research_approaches_score = normalize_and_weight(df['Research_Approaches_Jaccard_Similarity'], WEIGHT_RESEARCH_APPROACHES_JACCARD)
    # Calculate final score for Jaccard similarity
    #df['Score_Jaccard'] = research_areas_score + research_approaches_score
    df['Score_Jaccard'] = normalize_and_weight(df['Research_Approaches_Jaccard_Similarity'], 1)
    return df

### Strategy 2: Score based on extracted topic, objectives, and methods
def calculate_score_content(df):
#--------------------------------
    # Calculate normalized and weighted scores for topics/objectives and methods
    # topics_score = normalize_and_weight(df[['Topic_Similarity_Max', 'Objectives_Avg_Similarity_Avg']].mean(axis=1), WEIGHT_TOPICS)
    # methods_score = normalize_and_weight(df['Methods_Max_Similarity_Avg'], WEIGHT_METHODS)
    # Calculate final score for content-based similarity
    # df['Score_Content'] = topics_score + methods_score
    df['Score_Content'] = normalize_and_weight(df['Objectives_Max_Similarity_Max'], 1)
    return df

### Strategy 3: Score based on similarity of extracted MeSH terms
def calculate_score_mesh(df):
#--------------------------------
    # Use MeSH average similarity and MeSH semantic coverage score to create the final score
    #mesh_similarity_score = normalize_and_weight(df['MeSH_Avg_Similarity_Avg'], 0.5)
    #mesh_coverage_score = normalize_and_weight(df['MeSH_Semantic_Coverage_Score'], 0.5)
    #df['Score_MeSH'] = mesh_similarity_score + mesh_coverage_score
    df['Score_MeSH'] = normalize_and_weight(df['MeSH_Max_Similarity_Avg'], 1)
    return df

# Convert Expert_Number_Publications and Expert_Number_Citations columns to numbers
df_experts['NUMBER_PUBLICATIONS'] = df_experts['NUMBER_PUBLICATIONS'].apply(extract_number)
df_experts['NUMBER_CITATIONS'] = df_experts['NUMBER_CITATIONS'].apply(extract_number)

# Determine the threshold for "high" publications and citations
pub_threshold = np.percentile(df_experts['NUMBER_PUBLICATIONS'], NUM_PUBS_TOP_PERC_SENIORITY)
citations_threshold = np.percentile(df_experts['NUMBER_CITATIONS'], NUM_CITATIONS_TOP_PERC_SENIORITY)

# Determine if the expert is senior
df_experts['SENIOR_EXPERT'] = df_experts.apply(determine_seniority, axis=1)

# Load expert-project similarity scores.

df_content_similarity_scores = pd.read_pickle(FILE_PATH_EXPERT_PROJECT_CONTENT_SIMILARITY_SCORES)
df_mesh_scores = pd.read_pickle(FILE_PATH_EXPERT_PROJECT_MESH_SCORES)
df_jaccard_similarity_scores = pd.read_pickle(FILE_PATH_EXPERT_PROJECT_JACCARD_SIMILARITY)

### Combine data.

# Define the desired column order
column_order = [
    # Expert-specific columns
    'Expert_ID', 'Expert_Full_Name', 'Expert_Gender', 'Expert_Research_Types', 'Expert_Senior',
    'Expert_Experience_Reviewer', 'Expert_Experience_Panel', 'Expert_Number_Publications', 'Expert_Number_Citations',
    # Project-specific columns
    'Project_ID', 'Project_Title', 'Project_Research_Types',
    # Similarity scores (Jaccard, MeSH, Content)
    'Research_Areas_Jaccard_Similarity', 'Research_Approaches_Jaccard_Similarity',
    'Topic_Similarity_Max', 'Topic_Similarity_Avg',
    'Objectives_Max_Similarity_Max', 'Objectives_Max_Similarity_Avg',
    'Objectives_Avg_Similarity_Max', 'Objectives_Avg_Similarity_Avg',
    'Methods_Max_Similarity_Max', 'Methods_Max_Similarity_Avg',
    'Methods_Avg_Similarity_Max', 'Methods_Avg_Similarity_Avg',
    'Methods_Max_Similarity_Weighted_Max', 'Methods_Max_Similarity_Weighted_Avg',
    'Methods_Avg_Similarity_Weighted_Max', 'Methods_Avg_Similarity_Weighted_Avg',
    'MeSH_Semantic_Coverage_Score', 'MeSH_Max_Similarity_Max',
    'MeSH_Max_Similarity_Avg', 'MeSH_Avg_Similarity_Max',
    'MeSH_Avg_Similarity_Avg', 'MeSH_Max_Similarity_Weighted_Max',
    'MeSH_Max_Similarity_Weighted_Avg', 'MeSH_Avg_Similarity_Weighted_Max',
    'MeSH_Avg_Similarity_Weighted_Avg'
]

# Merge Jaccard similarity, MeSH scores, and content similarity into a single dataframe
df_combined_similarity_scores = pd.merge(df_jaccard_similarity_scores, df_mesh_scores, on=['Expert_ID', 'Project_ID'], how='inner')
df_combined_similarity_scores = pd.merge(df_combined_similarity_scores, df_content_similarity_scores, on=['Expert_ID', 'Project_ID'], how='inner')

# Loop through columns in the DataFrame and rename them if they have the prefix "Expert_"
new_column_names = {}
for col in df_combined_similarity_scores.columns:
    if col.startswith('Expert_') and col != 'Expert_ID':
        new_column_names[col] = col.replace('Expert_', '', 1)

# Apply the new column names to the DataFrame
df_combined_similarity_scores.rename(columns=new_column_names, inplace=True)

# Include columns from df_projects
df_projects_subset = df_projects[['ID', 'TITLE', 'RESEARCH_TYPE']].rename(columns={
    'ID': 'Project_ID',
    'TITLE': 'Project_Title',
    'RESEARCH_TYPE': 'Project_Research_Types'
})
df_combined_similarity_scores = pd.merge(df_combined_similarity_scores, df_projects_subset, on='Project_ID', how='left')

# Include columns from df_experts
df_experts_subset = df_experts[['ID', 'FULL_NAME', 'GENDER', 'RESEARCH_TYPES', 'SENIOR_EXPERT', 'EXPERIENCE_REVIEWER', 'EXPERIENCE_PANEL', 'NUMBER_PUBLICATIONS', 'NUMBER_CITATIONS']].rename(columns={
    'ID': 'Expert_ID',
    'FULL_NAME': 'Expert_Full_Name',
    'GENDER': 'Expert_Gender',
    'RESEARCH_TYPES': 'Expert_Research_Types',
    'SENIOR_EXPERT': 'Expert_Senior',
    'EXPERIENCE_REVIEWER': 'Expert_Experience_Reviewer',
    'EXPERIENCE_PANEL': 'Expert_Experience_Panel',
    'NUMBER_PUBLICATIONS': 'Expert_Number_Publications',
    'NUMBER_CITATIONS': 'Expert_Number_Citations'
})
df_combined_similarity_scores = pd.merge(df_combined_similarity_scores, df_experts_subset, on='Expert_ID', how='left')

# Reorder the DataFrame using the defined column order
df_combined_similarity_scores = df_combined_similarity_scores[column_order]

df_combined_similarity_scores.columns

df_manual_assignments = pd.read_csv(FILE_MANUAL_ASSIGNMENTS, sep='\t')

df_manual_assignments = df_manual_assignments[df_manual_assignments['Project_ID'].isin(best_predicted_projects)]

df_manual_assignments.head()

#df_output = pd.DataFrame()
#for project_id in assignments:
#  df_output = pd.concat([df_output, df_combined_similarity_scores[(df_combined_similarity_scores['Expert_Full_Name'].isin(assignments[project_id]))&(df_combined_similarity_scores['Project_ID']==project_id)]])
#df_output

df = df_combined_similarity_scores.copy()

# Ensure 'Project_ID' is a string in both DataFrames
df['Project_ID'] = df['Project_ID'].astype(str)
df_manual_assignments['Project_ID'] = df_manual_assignments['Project_ID'].astype(str)

# Strip whitespace and convert to lowercase for 'Expert_Full_Name' columns to ensure consistent formatting
df['Expert_Full_Name'] = df['Expert_Full_Name'].str.strip().str.lower()
df_manual_assignments['Expert_Full_Name'] = df_manual_assignments['Expert_Full_Name'].str.strip().str.lower()

# Add 'Manual_Assignment' column to df_manual_assignments for merging
df_manual_assignments['Manual_Assignment'] = 1

# Merge df with df_manual_assignments to add 'Manual_Assignment' column
df = df.merge(df_manual_assignments[['Project_ID', 'Expert_Full_Name', 'Manual_Assignment']],
              on=['Project_ID', 'Expert_Full_Name'],
              how='left')

# Fill NaN values in 'Manual_Assignment' with 0 (for pairs not in manual assignments)
df['Manual_Assignment'] = df['Manual_Assignment'].fillna(0).astype(int)

from sklearn.preprocessing import MinMaxScaler

# Initialize the MinMaxScaler
scaler = MinMaxScaler()

# Normalize 'Expert_Number_Publications'
df['Normalized_Publications'] = scaler.fit_transform(df[['Expert_Number_Publications']])

# Normalize 'Expert_Number_Citations'
df['Normalized_Citations'] = scaler.fit_transform(df[['Expert_Number_Citations']])

# Calculate 'Expertise_Level' as the average of normalized publications and citations
df['Expertise_Level'] = (df['Normalized_Publications'] + df['Normalized_Citations']) / 2

# Map 'yes' to 1 and 'no' to 0 for experience columns
experience_cols = ['Expert_Experience_Reviewer', 'Expert_Experience_Panel', 'Expert_Senior']
experience_mapping = {'yes': 1, 'no': 0}

for col in experience_cols:
    df[col] = df[col].map(experience_mapping)

# Calculate 'Experience_Score' as the average of experience indicators
df['Experience_Score'] = df[experience_cols].mean(axis=1)

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, roc_auc_score

# Define feature columns (exclude 'Manual_Assignment', 'Expert_ID', etc.)
feature_columns = [
'Research_Areas_Jaccard_Similarity',
       'Research_Approaches_Jaccard_Similarity', 'Topic_Similarity_Max',
       'Topic_Similarity_Avg', 'Objectives_Max_Similarity_Max',
       'Objectives_Max_Similarity_Avg', 'Objectives_Avg_Similarity_Max',
       'Objectives_Avg_Similarity_Avg', 'Methods_Max_Similarity_Max',
       'Methods_Max_Similarity_Avg', 'Methods_Avg_Similarity_Max',
       'Methods_Avg_Similarity_Avg', 'Methods_Max_Similarity_Weighted_Max',
       'Methods_Max_Similarity_Weighted_Avg',
       'Methods_Avg_Similarity_Weighted_Max',
       'Methods_Avg_Similarity_Weighted_Avg', 'MeSH_Semantic_Coverage_Score',
       'MeSH_Max_Similarity_Max', 'MeSH_Max_Similarity_Avg',
       'MeSH_Avg_Similarity_Max', 'MeSH_Avg_Similarity_Avg',
       'MeSH_Max_Similarity_Weighted_Max', 'MeSH_Max_Similarity_Weighted_Avg',
       'MeSH_Avg_Similarity_Weighted_Max', 'MeSH_Avg_Similarity_Weighted_Avg'
]

# Split data into features (X) and target (y)
X = df[feature_columns]
y = df['Manual_Assignment']

# Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)

# Initialize and train the logistic regression model
model = LogisticRegression(max_iter=1000)
model.fit(X_train, y_train)

# Predict on the test set
y_pred = model.predict(X_test)
y_pred_proba = model.predict_proba(X_test)[:, 1]

# Evaluate the model
print("Classification Report:")
print(classification_report(y_test, y_pred))

roc_auc = roc_auc_score(y_test, y_pred_proba)
print(f"ROC AUC Score: {roc_auc:.4f}")

# Get feature coefficients
coefficients = pd.DataFrame({
    'Feature': feature_columns,
    'Coefficient': model.coef_[0]
})

print("\nFeature Coefficients:")
print(coefficients.sort_values(by='Coefficient', ascending=False))

# Predict probabilities for all data
df['Assignment_Probability'] = model.predict_proba(X)[:, 1]

# Rank experts within each project
df['Rank'] = df.groupby('Project_ID')['Assignment_Probability'].rank(method='dense', ascending=False)

# Sort the DataFrame
df = df.sort_values(by=['Project_ID', 'Rank'])
df.reset_index(drop=True, inplace=True)

df_manual_assignments = pd.read_csv(FILE_MANUAL_ASSIGNMENTS, sep='\t')

# Initialize a list to store the results
percentage_in_top20 = []

# Get the list of unique projects
project_ids = df['Project_ID'].unique()

for project_id in project_ids:
    # Get the subset of data for the current project
    project_df = df[df['Project_ID'] == project_id]

    # Total number of manually assigned experts for this project
    total_manual_assigned = project_df['Manual_Assignment'].sum()

    # Get the top-20 experts based on the automatic ranking
    top20_experts = project_df[project_df['Rank'] <= 20]

    # Number of manually assigned experts in the top-20
    manual_in_top20 = top20_experts['Manual_Assignment'].sum()

    # Calculate the percentage
    if total_manual_assigned > 0:
        percentage = (manual_in_top20 / total_manual_assigned) * 100
    else:
        percentage = np.nan  # Handle cases where there are no manual assignments

    # Append the results
    percentage_in_top20.append({
        'Project_ID': project_id,
        'Total_Manual_Assigned': total_manual_assigned,
        'Manual_In_Top20': manual_in_top20,
        'Percentage_In_Top20': percentage
    })

# Convert the results to a DataFrame for easy viewing
percentage_df = pd.DataFrame(percentage_in_top20)

# Display the results
print("\nPercentage of Manually Assigned Experts in Top-20 Automatic Assignments:")
print(percentage_df.to_string(index=False))

best_predicted_projects = percentage_df[percentage_df['Percentage_In_Top20']>60]['Project_ID'].to_list()

best_predicted_projects

# Total number of manually assigned experts across all projects
total_manual_assigned_overall = df['Manual_Assignment'].sum()

# Total number of manually assigned experts in the top-20 across all projects
manual_in_top20_overall = df[df['Rank'] <= 20]['Manual_Assignment'].sum()

# Calculate the overall percentage
overall_percentage = (manual_in_top20_overall / total_manual_assigned_overall) * 100

print(f"\nOverall Percentage of Manually Assigned Experts in Top-20: {overall_percentage:.2f}%")

for project_id in assignments:
  df_subset_project = df_top_experts[df_top_experts['Project_ID']==project_id].copy()
  df_subset_project['AQuAS'] = df_subset_project.apply(lambda row: row['Expert_Full_Name'] in assignments[project_id], axis=1)
  display(df_subset_project[['Project_ID', 'Expert_Full_Name','Expert_Research_Types','Project_Research_Types','Adjusted_Combined_Score','Rank','AQuAS']])



### Calculate scores for each strategy
df_scored = df_combined_similarity_scores.copy()

# Calculate Jaccard-based score
df_scored = calculate_score_jaccard(df_scored)

# Calculate content-based score
df_scored = calculate_score_content(df_scored)

# Calculate MeSH-based score
df_scored = calculate_score_mesh(df_scored)

### Add ranking columns for each score strategy

scores_columns = ['Score_Jaccard', 'Score_Content', 'Score_MeSH']
ranks_columns = [column.replace('Score', 'Rank') for column in scores_columns]

# !!!!!!!!!!!!!!!!!! TEST !!!!!!!!!!!!!!!!!
df_scored['Score_Average'] = df_scored[scores_columns].mean(axis=1)
#df_scored['Score_Weighted_Average'] = (df_scored['Score_MeSH']*1 + df_scored['Score_Jaccard']*1 + df_scored['Score_Content']*1.25)/3

scores_columns.append('Score_Average')
ranks_columns.append('Rank_Average')

#scores_columns.append('Score_Weighted_Average')
#ranks_columns.append('Rank_Weighted_Average')

for column in scores_columns:
  # Rank experts for each project based on the column score
  df_scored[column.replace('Score', 'Rank')] = df_scored.groupby('Project_ID')[column].rank(ascending=False, method='first', na_option='bottom')

### Define the columns to keep
columns_to_keep = [col for col in df_scored.columns if col.startswith('Expert_') or col.startswith('Project_')]
columns_to_keep += scores_columns
columns_to_keep += ranks_columns

# Filter the dataframe to include only the specified columns
df_filtered = df_scored[columns_to_keep]

#project_id = '8'
#df_filtered[(df_filtered['Expert_Full_Name'].isin(assignments[project_id]))&(df_filtered['Project_ID']==project_id)]

for project_id in assignments:
  df_selected = df_filtered[((df_filtered['Rank_Average']<=15)|(df_filtered['Expert_Full_Name']).isin(assignments[project_id]))&(df_filtered['Project_ID']==project_id)].copy()
  df_selected['AQuAS'] = df_selected.apply(lambda row: row['Expert_Full_Name'] in assignments[project_id], axis=1)
  display(df_selected[['Project_ID', 'Expert_Full_Name','Expert_Research_Types','Project_Research_Types']+ranks_columns+['AQuAS']].sort_values(by='Rank_Average'))
  print('------------------------------------------------')

#df_filtered[(df_filtered['Rank_Jaccard']<=10)&(df_filtered['Project_ID']=='19')]

### Save the dataframe for analysis
df_filtered.to_csv(FILE_PATH_COMBINED_SCORES, sep='\t', index=False)