# -*- coding: utf-8 -*-
"""2a-publication_enrichment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RLJB6WQAztjmfR7y4msofl56v6GmHb7l

### **IMPORTS**
"""

!pip install -q abbreviations

import os
import sys
import torch
import pandas as pd

from google.colab import drive
from tqdm import tqdm
from transformers import AutoModelForCausalLM, AutoModelForTokenClassification, AutoTokenizer, pipeline
from abbreviations import schwartz_hearst

"""### **SETTINGS**"""

### Test size if we want to process a subset for testing. Set as 0 to ignore.
TEST_SIZE_PUBLICATIONS = 10

# Whether to save processed data (overriding existing).
SAVE_PUBLICATIONS = True

"""#### GOOGLE DRIVE"""

### Mount drive.
drive.mount('/content/drive')

"""####IMPORT CONFIGURATIONS AND FUNCTIONS"""

# Add the "config" and "functions" folders to the Python path.
sys.path.append('/content/drive/MyDrive/1_Current_projects_SIRIS/2024AQUAS-ReviewerMatcher/Implementation/Notebooks/config')
sys.path.append('/content/drive/MyDrive/1_Current_projects_SIRIS/2024AQUAS-ReviewerMatcher/Implementation/Notebooks/functions')

# Import general configurations and those needed to call the LLM and process its responses.
from config_general import *
from config_llm import *

# Import functions to call the LLM and process its responses.
from functions_llm import *
from functions_read_data import *

print(f'Processing {CALL_NAME}')
print(f'Output path: {OUTPUT_PATH_PUBLICATIONS}')

"""#### INITIALIZATIONS"""

tqdm.pandas()
torch.random.manual_seed(0)

### Read prompts from files.

with open(f'{CODE_PATH}/{FILE_PROMPT_CONTENT}') as f:
  PROMPT_CONTENT = f.read()

with open(f'{CODE_PATH}/{FILE_PROMPT_METHODS}') as f:
  PROMPT_METHODS = f.read()

### Generative pipeline used to enrich projects and expert publications.

# Load model, tokenizer.
model_generative = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME_GENERATIVE,
    device_map='cuda',
    torch_dtype='auto',
    trust_remote_code=True,
  )
tokenizer_generative = AutoTokenizer.from_pretrained(MODEL_NAME_GENERATIVE)

# Generate pipeline.
pipeline_generation = pipeline(
    'text-generation',
    model=model_generative,
    tokenizer=tokenizer_generative,
  )

# Set arguments.
generation_args = {
    'max_new_tokens': MAX_NEW_TOKENS,
    'return_full_text': False,
    'temperature': 0.0,
    'do_sample': False,
  }

"""### **ADD EXTRACTED CONTENT (GOAL, OBJECTIVES, METHODS) TO PUBLICATIONS**"""

### Load .
df_publications = pd.read_pickle(OUTPUT_PATH_PUBLICATIONS)

if TEST_SIZE_PUBLICATIONS:
  df_publications = df_publications.head(TEST_SIZE_PUBLICATIONS)

"""#### Get research topics, objectives, and coarse-grained methods with generative model."""

### Get research area, objective, coarse-grained methods for publications.

# Get abbreviations.
df_publications['ABBREVIATIONS'] = df_publications.apply(
    lambda row: schwartz_hearst.extract_abbreviation_definition_pairs(doc_text=f'{row["TITLE_PUBMED"]}. {row["ABSTRACT_PUBMED"]}'),
    axis=1
  )

# Get summary sentences with abbreviations expanded.
df_publications[EXTRACTED_CONTENT_COLUMNS] = df_publications.progress_apply(lambda row:
    pd.Series(
      expand_abbreviations(
        extract_content(
            pipeline_generation,
            generation_args,
            PROMPT_CONTENT.format(title=row['TITLE_PUBMED'], abstract=row['ABSTRACT_PUBMED']),
            DEFAULT_RESPONSE_CONTENT,
            max_retries=MAX_RETRIES,
            retry_delay=RETRY_DELAY
        ),
        row['ABBREVIATIONS']
      )
    ),
    axis=1
  )

df_publications[['TITLE_PUBMED'] + EXTRACTED_CONTENT_COLUMNS].head(3)

"""#### Get fine-grained methods with generative model."""

### Get fine-grained methods for publications.

if not ONE_STEP_FINE_GRAINED_METHODS:

  df_publications[COLUMNS_FINE_GRAINED_METHODS] = df_publications.progress_apply(lambda row:
      pd.Series(
        expand_abbreviations(
          extract_content(
              pipeline_generation,
              generation_args,
              PROMPT_METHODS.format(title=row['TITLE_PUBMED'], methods='- ' + '\n- '.join(value_as_list(row['METHODS'], SEPARATOR_VALUES_OUTPUT))),
              DEFAULT_RESPONSE_METHODS,
              max_retries=MAX_RETRIES,
              retry_delay=RETRY_DELAY
          ),
          row['ABBREVIATIONS']
        )
      ),
      axis=1
    )

df_publications[['ID', 'TITLE_PUBMED'] + EXTRACTED_CONTENT_COLUMNS + COLUMNS_FINE_GRAINED_METHODS].head(3)

### Flatten lists and save publications.

if len(df_publications)>0 and SAVE_PUBLICATIONS:
  if TEST_SIZE_PUBLICATIONS and f'_test_{TEST_SIZE_PUBLICATIONS}' not in OUTPUT_PATH_PUBLICATIONS:
    OUTPUT_PATH_PUBLICATIONS = OUTPUT_PATH_PUBLICATIONS.replace('.pkl', f'_test_{TEST_SIZE_PUBLICATIONS}.pkl')
  df_publications = flatten_lists(df_publications, SEPARATOR_VALUES_OUTPUT)
  df_publications.to_pickle(OUTPUT_PATH_PUBLICATIONS)
  print(f'Publications saved to file {OUTPUT_PATH_PUBLICATIONS}')