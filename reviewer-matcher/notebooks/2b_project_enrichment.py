# -*- coding: utf-8 -*-
"""2b-project_enrichment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1luO-6nk1oHWjiAr2usmCECvo36dlPGel

### **IMPORTS**
"""

!pip install -q abbreviations

import os
import sys
import pandas as pd

from google.colab import drive
from tqdm import tqdm
from abbreviations import schwartz_hearst

"""### **SETTINGS**"""

### Test size if we want to process a subset for testing. Set as 0 to ignore.
TEST_SIZE_PROJECTS = 10

# Whether to save processed data (overriding existing).
SAVE_PROJECTS = True

"""#### GOOGLE DRIVE"""

### Mount drive.
drive.mount('/content/drive')

"""####IMPORT CONFIGURATIONS AND FUNCTIONS"""

# Add the "config" and "functions" folders to the Python path.
sys.path.append('/content/drive/MyDrive/1_Current_projects_SIRIS/2024AQUAS-ReviewerMatcher/Implementation/Notebooks/config')
sys.path.append('/content/drive/MyDrive/1_Current_projects_SIRIS/2024AQUAS-ReviewerMatcher/Implementation/Notebooks/functions')

# Import general configurations and those needed to call the LLM and process its responses.
from config_general import *
from config_llm import *

# Import functions to call the LLM and process its responses.
from functions_llm import *
from functions_read_data import *

if not USE_EXTERNAL_LLM_MODEL:
  from transformers import AutoModelForCausalLM, AutoModelForTokenClassification, AutoTokenizer, pipeline
  import torch
  torch.random.manual_seed(0)

# Some external models support requests for returning JSON responses - in the other cases we parse the response and get the JSON from it.
JSON_RESPONSE = USE_EXTERNAL_LLM_MODEL and MODEL_NAME_GENERATIVE in MODELS_JSON_RESPONSE

print(f'CALL_NAME = {CALL_NAME}')
print(f'MODEL_NAME_GENERATIVE = {MODEL_NAME_GENERATIVE}')
print(f'JSON_RESPONSE = {JSON_RESPONSE}')

"""#### INITIALIZATIONS"""

tqdm.pandas()

### Read prompts from files.

with open(f'{CODE_PATH}/{FILE_PROMPT_TOPICS}') as f:
  PROMPT_TOPICS = f.read()

with open(f'{CODE_PATH}/{FILE_PROMPT_APPROACHES}') as f:
  PROMPT_APPROACHES = f.read()

with open(f'{CODE_PATH}/{FILE_PROMPT_CONTENT}') as f:
  PROMPT_CONTENT = f.read()

with open(f'{CODE_PATH}/{FILE_PROMPT_METHODS}') as f:
  PROMPT_METHODS = f.read()

### Generative pipeline used to enrich projects and expert publications.

# Set arguments.

if USE_EXTERNAL_LLM_MODEL:
  pipeline_generation = None
  generation_args = {
      'model': MODEL_NAME_GENERATIVE,
      'external_model_url': EXTERNAL_MODEL_URL,
      'api_key': EXTERNAL_MODEL_API_KEY,
      'max_tokens': MAX_NEW_TOKENS,
      'temperature': 0.0,
      'top_p': 1.0,
      'echo': False
    }
else:
  generation_args = {
      'max_new_tokens': MAX_NEW_TOKENS,
      'return_full_text': False,
      'temperature': 0.0,
      'do_sample': False,
    }
  # Load model, tokenizer.
  model_generative = AutoModelForCausalLM.from_pretrained(
      MODEL_NAME_GENERATIVE,
      device_map='cuda',
      torch_dtype='auto',
      trust_remote_code=True,
    )
  tokenizer_generative = AutoTokenizer.from_pretrained(MODEL_NAME_GENERATIVE)
  # Generate pipeline.
  pipeline_generation = pipeline(
      'text-generation',
      model=model_generative,
      tokenizer=tokenizer_generative,
    )

"""#### READ PROJECTS"""

### Load projects.
df_projects = pd.read_pickle(OUTPUT_PATH_PROJECTS)

if TEST_SIZE_PROJECTS:
  df_projects = df_projects.head(TEST_SIZE_PROJECTS)

"""### **CLASSIFY PROJECTS WITH TOPICS / APPROACHES FROM EXPERTS DATA**"""

df_projects.head(5)

### Load already processed experts data.
df_experts = pd.read_pickle(OUTPUT_PATH_EXPERTS)

### Get values for expert columns to tag projects with.
VALUES_EXPERTS_COLUMNS = {}

for expert_column in EXTRACT_VALUES_EXPERTS_COLUMNS:
  column_values = df_experts[expert_column].dropna().apply(lambda x: x.split('|'))
  VALUES_EXPERTS_COLUMNS[expert_column] = list(set(
      value.strip(f'\'"{string.whitespace}')
      for sublist in column_values
      for value in sublist
      if value.strip(f'\'"{string.whitespace}')
  ))

VALUES_EXPERTS_COLUMNS['RESEARCH_AREAS']

VALUES_EXPERTS_COLUMNS['RESEARCH_APPROACHES']

"""#### Label projects with research areas with generative model."""

### Add information about pre-defined topics.

generation_args['temperature'] = TEMPERATURE_CLASSIFICATION

research_areas_projects = df_projects.progress_apply(lambda row:
      pd.Series(
        label_by_topic(
          pipeline_generation,
          generation_args,
          PROMPT_TOPICS,
          title=row['TITLE'],
          abstract=row['ABSTRACT'],
          topics=VALUES_EXPERTS_COLUMNS['RESEARCH_AREAS'],
          max_topics=MAX_NUMBER_TOPICS_PER_PROMPT,
          max_retries=MAX_RETRIES,
          retry_delay=RETRY_DELAY,
          json_response=JSON_RESPONSE
        )
      ),
    axis=1
  )

# Combine output and add column.
df_projects['RESEARCH_AREAS'] = research_areas_projects.apply(
    lambda row: extract_and_combine_responses(row, VALUES_EXPERTS_COLUMNS['RESEARCH_AREAS'], SEPARATOR_VALUES_OUTPUT),
    axis=1
)

"""#### Label projects with research approaches with generative model."""

### Add information about pre-defined approaches.

generation_args['temperature'] = TEMPERATURE_CLASSIFICATION

research_approaches_projects = df_projects.progress_apply(lambda row:
      pd.Series(
        label_by_topic(
          pipeline_generation,
          generation_args,
          PROMPT_APPROACHES,
          title=row['TITLE'],
          abstract=row['ABSTRACT'],
          topics=VALUES_EXPERTS_COLUMNS['RESEARCH_APPROACHES'],
          max_topics=MAX_NUMBER_TOPICS_PER_PROMPT,
          max_retries=MAX_RETRIES,
          retry_delay=RETRY_DELAY,
          json_response=JSON_RESPONSE
        )
      ),
    axis=1
  )

# Combine output and add column.
df_projects['RESEARCH_APPROACHES'] = research_approaches_projects.apply(
    lambda row: extract_and_combine_responses(row, VALUES_EXPERTS_COLUMNS['RESEARCH_APPROACHES'], SEPARATOR_VALUES_OUTPUT),
    axis=1
)

df_projects[['TITLE', 'RESEARCH_AREAS', 'RESEARCH_APPROACHES']].head()

"""### **ADD EXTRACTED CONTENT (GOAL, OBJECTIVES, METHODS) TO PUBLICATIONS**

#### Get projects' main research topics, objectives, and coarse-grained models with generative model.
"""

### Get research area, objective, methods.

generation_args['temperature'] = TEMPERATURE_GENERATION

if JSON_RESPONSE:
  response_format = {
    'schema': generate_research_summary_schema(),
    'type': 'json_object'
  }
  generation_args['parameters'] = {'json_mode': True}
  generation_args['response_format'] = response_format

# Get abbreviations.
df_projects['ABBREVIATIONS'] = df_projects.apply(
    lambda row: schwartz_hearst.extract_abbreviation_definition_pairs(doc_text=f'{row["TITLE"]}. {row["ABSTRACT"]}'),
    axis=1
  )

# Get summary sentences with abbreviations expanded.
df_projects[EXTRACTED_CONTENT_COLUMNS] = df_projects.progress_apply(lambda row:
    pd.Series(
      expand_abbreviations(
        extract_content(
            pipeline_generation,
            generation_args,
            PROMPT_CONTENT.format(title=row['TITLE'], abstract=row['ABSTRACT']),
            DEFAULT_RESPONSE_CONTENT,
            max_retries=MAX_RETRIES,
            retry_delay=RETRY_DELAY
        ),
        row['ABBREVIATIONS']
      )
    ),
    axis=1
  )

df_projects[['TITLE'] + EXTRACTED_CONTENT_COLUMNS].head()

"""#### Get projects' fine-grained methods with generative model."""

if not ONE_STEP_FINE_GRAINED_METHODS:

  generation_args['temperature'] = TEMPERATURE_GENERATION

  if JSON_RESPONSE:
    response_format = {
      'schema': generate_method_classification_schema(),
      'type': 'json_object'
    }
    generation_args['parameters'] = {'json_mode': True}
    generation_args['response_format'] = response_format

  df_projects[COLUMNS_FINE_GRAINED_METHODS] = df_projects.progress_apply(lambda row:
      pd.Series(
        expand_abbreviations(
          extract_content(
              pipeline_generation,
              generation_args,
              PROMPT_METHODS.format(title=row['TITLE'], methods='- ' + '\n- '.join(value_as_list(row['METHODS'], SEPARATOR_VALUES_OUTPUT))),
              DEFAULT_RESPONSE_METHODS,
              max_retries=MAX_RETRIES,
              retry_delay=RETRY_DELAY
          ),
          row['ABBREVIATIONS']
        )
      ),
      axis=1
    )

df_projects[['METHODS'] + COLUMNS_FINE_GRAINED_METHODS].head(10)

### Flatten lists and save output.
if len(df_projects)>0 and SAVE_PROJECTS:
  if TEST_SIZE_PROJECTS and f'_test_{TEST_SIZE_PROJECTS}' not in OUTPUT_PATH_PROJECTS:
    OUTPUT_PATH_PROJECTS = OUTPUT_PATH_PROJECTS.replace('.pkl', f'_test_{TEST_SIZE_PROJECTS}.pkl')
  df_projects = flatten_lists(df_projects, SEPARATOR_VALUES_OUTPUT)
  df_projects.to_pickle(OUTPUT_PATH_PROJECTS)
  print(f'Projects saved to file {OUTPUT_PATH_PROJECTS}')

df_projects.head(10)

