# -*- coding: utf-8 -*-
"""8-get_linear_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QU2a1g69MEgIG8LGYd6hnBxot4ob5yTo

**This notebook is for exploration purposes - part of its contents will be included in other notebooks**
"""

# TODO: Modify files before saving in the notebook that generates the data and saves the Pickle files so this is not necessary.
!pip install -q abbreviations

import pandas as pd
import re
import numpy as np
import abbreviations

from google.auth import default
from google.colab import auth
from google.colab import drive

from tqdm import tqdm

#from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, roc_auc_score
from sklearn.model_selection import GridSearchCV

from google.colab import drive
drive.mount('/content/drive')

### Input/output paths.

# Specific call folder (used to retrieve the configuration, URLs, etc).
CALL_NAME = '2021-Salut Mental'

# Bath path for all the sample data.
BASE_PATH = '/content/drive/MyDrive/1_Current_projects_SIRIS/2024AQUAS-ReviewerMatcher'

# Code path.
CODE_PATH = f'{BASE_PATH}/Implementation/Notebooks'

# Data path.
DATA_PATH = f'{BASE_PATH}/Implementation/Data'

# Input files.
FILE_PATH_COMBINED_SCORES = f'{DATA_PATH}/{CALL_NAME}/scores/expert_projects_normalized_scores.tsv'

# For evaluation
FILE_MANUAL_ASSIGNMENTS = f'{DATA_PATH}/{CALL_NAME}/manual_assignments.tsv'

# This is just to get some examples.
FILE_PROJECTS = f'{DATA_PATH}/{CALL_NAME}/projects.pkl'
FILE_EXPERTS = f'{DATA_PATH}/{CALL_NAME}/experts.pkl'

FILE_EXPERT_PROJECT_JACCARD_SIMILARITY = f'{DATA_PATH}/{CALL_NAME}/scores/expert_project_jaccard_similarity_scores.pkl'

FILE_PUBLICATIONS_PROJECTS_CONTENT_SCORES = f'{DATA_PATH}/{CALL_NAME}/scores/publications_projects_content_similarity_scores.pkl'
FILE_PUBLICATIONS_PROJECTS_MESH_SCORES = f'{DATA_PATH}/{CALL_NAME}/scores/publications_projects_mesh_scores.pkl'

FILE_PROJECTS_MESH = f'{DATA_PATH}/{CALL_NAME}/projects_with_mesh.pkl'
FILE_PUBLICATIONS_MESH = f'{DATA_PATH}/{CALL_NAME}/expert_publications_with_mesh.pkl'

df_projects = pd.read_pickle(FILE_PROJECTS)
df_experts = pd.read_pickle(FILE_EXPERTS)

df_publications_mesh = pd.read_pickle(FILE_PUBLICATIONS_MESH)
df_projects_mesh = pd.read_pickle(FILE_PROJECTS_MESH)

df_publications_projects_content_scores = pd.read_pickle(FILE_PUBLICATIONS_PROJECTS_CONTENT_SCORES)
df_publications_projects_mesh_scores = pd.read_pickle(FILE_PUBLICATIONS_PROJECTS_MESH_SCORES)

df_jaccard = pd.read_pickle(FILE_EXPERT_PROJECT_JACCARD_SIMILARITY)

TRAIN_ASSIGNMENT = 'Final_Assignment'

### Settings

SEPARATOR_VALUES = '|'

# Load data

df_combined_scores = pd.read_csv(FILE_PATH_COMBINED_SCORES, sep='\t')
df_manual_assignments = pd.read_csv(FILE_MANUAL_ASSIGNMENTS, sep='\t').fillna(0)

# Ensure 'Project_ID' is a string in both DataFrames
df_combined_scores['Project_ID'] = df_combined_scores['Project_ID'].astype(str)
df_manual_assignments['Project_ID'] = df_manual_assignments['Project_ID'].fillna(0).astype(int).astype(str)

df_final_assignments = df_manual_assignments[df_manual_assignments['Final_Assignment']==1]

df_combined_scores.columns

# Make copy of scores dataframe to process.
df = df_combined_scores.copy()

df_manual_assignments.head()

# Add 'Manual_Assignment' column to df_manual_assignments for merging
df_manual_assignments['Manual_Assignment'] = 1

# Merge df with df_manual_assignments to add 'Manual_Assignment' column
df = df.merge(df_manual_assignments[['Project_ID', 'Expert_Full_Name', 'Manual_Assignment']],
              on=['Project_ID', 'Expert_Full_Name'],
              how='left')

# Fill NaN values in 'Manual_Assignment' with 0 (for pairs not in manual assignments)
df['Manual_Assignment'] = df['Manual_Assignment'].fillna(0).astype(int)

# Merge df with df_manual_assignments to add 'Manual_Assignment' column
df = df.merge(df_final_assignments[['Project_ID', 'Expert_Full_Name', 'Final_Assignment']],
              on=['Project_ID', 'Expert_Full_Name'],
              how='left')

# Fill NaN values in 'Manual_Assignment' with 0 (for pairs not in manual assignments)
df['Final_Assignment'] = df['Final_Assignment'].fillna(0).astype(int)

# Define feature columns (exclude 'Manual_Assignment', 'Expert_ID', etc.)
feature_columns = [
  'Expert_Seniority',
  'Research_Type_Similarity_Score',
  'Research_Areas_Jaccard_Similarity',
  'Research_Approaches_Jaccard_Similarity',
  'Topic_Similarity_Max',
  'Topic_Similarity_Avg',
  'Objectives_Max_Similarity_Max',
  'Objectives_Max_Similarity_Avg',
  'Objectives_Avg_Similarity_Max',
  'Objectives_Avg_Similarity_Avg',
  'Methods_Max_Similarity_Max',
  'Methods_Max_Similarity_Avg',
  'Methods_Avg_Similarity_Max',
  'Methods_Avg_Similarity_Avg',
  'Methods_Max_Similarity_Weighted_Max',
  'Methods_Max_Similarity_Weighted_Avg',
  'Methods_Avg_Similarity_Weighted_Max',
  'Methods_Avg_Similarity_Weighted_Avg',
  'MeSH_Semantic_Coverage_Score',
  'MeSH_Max_Similarity_Max',
  'MeSH_Max_Similarity_Avg',
  'MeSH_Avg_Similarity_Max',
  'MeSH_Avg_Similarity_Avg',
  'MeSH_Max_Similarity_Weighted_Max',
  'MeSH_Max_Similarity_Weighted_Avg',
  'MeSH_Avg_Similarity_Weighted_Max',
  'MeSH_Avg_Similarity_Weighted_Avg'
]

"""
feature_columns = [
  'Expert_Seniority',
  'Research_Type_Similarity_Score',
  'Research_Approaches_Jaccard_Similarity',
  'Topic_Similarity_Max',
  'Objectives_Max_Similarity_Max',
  'MeSH_Avg_Similarity_Max',
]
"""

from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# List of feature groups to apply PCA on
feature_groups = {
    'MeSH': [
        'MeSH_Semantic_Coverage_Score',
        'MeSH_Max_Similarity_Max',
        'MeSH_Max_Similarity_Avg',
        'MeSH_Avg_Similarity_Max',
        'MeSH_Avg_Similarity_Avg',
        'MeSH_Max_Similarity_Weighted_Max',
        'MeSH_Max_Similarity_Weighted_Avg',
        'MeSH_Avg_Similarity_Weighted_Max',
        'MeSH_Avg_Similarity_Weighted_Avg'
    ],
    'Topic': [
        'Topic_Similarity_Max',
        'Topic_Similarity_Avg'
    ],
    'Objectives': [
        'Objectives_Max_Similarity_Max',
        'Objectives_Max_Similarity_Avg',
        'Objectives_Avg_Similarity_Max',
        'Objectives_Avg_Similarity_Avg'
    ],
    'Methods': [
        'Methods_Max_Similarity_Max',
        'Methods_Max_Similarity_Avg',
        'Methods_Avg_Similarity_Max',
        'Methods_Avg_Similarity_Avg',
        'Methods_Max_Similarity_Weighted_Max',
        'Methods_Max_Similarity_Weighted_Avg',
        'Methods_Avg_Similarity_Weighted_Max',
        'Methods_Avg_Similarity_Weighted_Avg'
    ]
}

#feature_groups['ALL_Semantic'] =  feature_groups['MeSH'] + feature_groups['Topic'] + feature_groups['Objectives'] + feature_groups['Methods']

# Apply PCA to each feature group
pca_results = {}
scalers = {}

for group_name, group_features in feature_groups.items():
    # Standardize the features
    scaler = StandardScaler()
    X_group_standardized = scaler.fit_transform(df[group_features])
    scalers[group_name] = scaler

    # Apply PCA, retaining enough components to explain 90% of the variance
    #pca = PCA(n_components=0.90)
    pca = PCA(n_components=1)
    X_group_pca = pca.fit_transform(X_group_standardized)
    pca_results[group_name] = pca

    # Create column names for the PCA components
    pca_columns = [f'{group_name}_PCA_{i+1}' for i in range(X_group_pca.shape[1])]
    df_pca = pd.DataFrame(X_group_pca, columns=pca_columns)

    # Add the PCA components to the original DataFrame
    df = pd.concat([df, df_pca], axis=1)

# After PCA, keep the key components and original unrelated features
reduced_feature_columns = [
    'Expert_Seniority',
    'Research_Type_Similarity_Score',
    'Research_Areas_Jaccard_Similarity',
    'Research_Approaches_Jaccard_Similarity'
] + [col for col in df.columns if col.startswith(tuple(feature_groups.keys())) and 'PCA' in col]

reduced_feature_columns

# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!! TEST !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
feature_columns = reduced_feature_columns

# !!!! TEST - train with subset !!!!

df_train = df

X_train = df_train[feature_columns]
y_train = df_train[TRAIN_ASSIGNMENT]

# Initialize the logistic regression model
model = LogisticRegression(max_iter=1000)

# Hyperparameters to tune
param_grid = {
    'C': [0.01, 0.1, 1, 10, 100],  # Regularization strength
    'penalty': ['l1', 'l2'],       # Regularization type
    'solver': ['liblinear', 'saga'] # Solvers that support L1/L2 penalties
}

# Set up cross-validation with GridSearchCV
grid_search = GridSearchCV(model, param_grid, cv=5, scoring='roc_auc', verbose=2, n_jobs=-1)
grid_search.fit(X_train, y_train)

# Get best model and parameters from GridSearchCV
print("Best parameters found:", grid_search.best_params_)
best_model = grid_search.best_estimator_

X_pred = df[feature_columns]

# Predict the probabilities on the entire dataset
y_pred_proba = best_model.predict_proba(X_pred)[:, 1]

# Evaluate the model on the whole dataset
#roc_auc = roc_auc_score(y, y_pred_proba)
#print(f"ROC AUC Score on the entire dataset: {roc_auc:.4f}")

# Add prediction probabilities to the dataframe
df['Assignment_Probability'] = y_pred_proba

# Rank experts within each project
df['Rank_Assignment_Probability'] = df.groupby('Project_ID')['Assignment_Probability'].rank(method='dense', ascending=False)

# Sort the DataFrame
df = df.sort_values(by=['Project_ID', 'Rank_Assignment_Probability']).reset_index(drop=True)

# Display the results
df[['Project_ID', 'Expert_Full_Name', 'Project_Research_Types', 'Expert_Research_Types', 'Assignment_Probability', 'Rank_Assignment_Probability']]

df.sort_values(by=['Project_ID','Rank_Assignment_Probability'])[['Project_ID','Expert_ID','Expert_Full_Name','Expert_Gender','Expert_Research_Types','Expert_Seniority','Project_Research_Types','Assignment_Probability','Rank_Assignment_Probability']].head(10)

example_experts = ['Cecilia Essau','Amy Naca Mendenhall','Robert F. Valois']
df[(df['Project_ID']=='100')&(df['Expert_Full_Name'].isin(example_experts))][['Project_ID','Expert_ID','Expert_Full_Name','Expert_Gender','Expert_Research_Types','Expert_Seniority','Project_Research_Types','Assignment_Probability','Rank_Assignment_Probability']]

# Get the coefficients of the best model
coefficients = best_model.coef_[0]  # Assuming it's not a multi-class model

# Create a DataFrame to display the coefficients along with feature names
coefficients_df = pd.DataFrame({
    'Feature': feature_columns,  # Your list of feature names
    'Coefficient': coefficients
})

# Sort the coefficients to see the most important features
coefficients_df = coefficients_df.sort_values(by='Coefficient', ascending=False)

# Display the coefficients
print("\nCoefficients of the Best Model:")
print(coefficients_df)

from sklearn.inspection import permutation_importance

result = permutation_importance(best_model, X_train, y_train, n_repeats=10, random_state=42, scoring='roc_auc')
permutation_importances = pd.DataFrame({
    'Feature': feature_columns,
    'Importance': result.importances_mean
}).sort_values(by='Importance', ascending=False)

print('Permutation feature importances')
print(permutation_importances)

from sklearn.feature_selection import RFE

rfe = RFE(estimator=model, n_features_to_select=5)
rfe.fit(X_train, y_train)

informative_features_rfe = pd.DataFrame({
    'Feature': feature_columns,
    'Rank': rfe.ranking_
}).sort_values(by='Rank')

print('Recursive Feature Elimination (RFE)')
print(informative_features_rfe)

from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

feature_importances = pd.DataFrame({
    'Feature': feature_columns,
    'Importance': model.feature_importances_
}).sort_values(by='Importance', ascending=False)

print('Tree-based feature importance')
print(feature_importances)

import itertools

# Define the base weights
selected_features = {
    'Expert_Seniority': 0.5,
    'Research_Type_Similarity_Score': 0.75,
    'Research_Approaches_Jaccard_Similarity': 2.75,
    'Research_Areas_Jaccard_Similarity': 1.0,
    'MeSH_PCA_1': 0.5,
    'Topic_PCA_1': 0.75,
    'Objectives_PCA_1': 0.25,
    'Methods_PCA_1': 0
}

# Set the increment and range for exploration
increment = 0.25
variation_range = 0.25

# Generate the ranges for each weight (ensuring non-negative values)
weight_ranges = {}
for feature, base_weight in selected_features.items():
    min_weight = max(0, base_weight - variation_range)  # Ensure weights stay non-negative
    max_weight = base_weight + variation_range
    weight_ranges[feature] = np.arange(min_weight, max_weight + increment, increment)

# Generate all combinations of weights
all_combinations = list(itertools.product(*weight_ranges.values()))

# Create a DataFrame to store the combinations
weight_combinations_df = pd.DataFrame(all_combinations, columns=selected_features.keys())

# Display the total number of combinations and a sample of the combinations
print(f"Total number of weight combinations: {len(weight_combinations_df)}")
weight_combinations_df

# The idea is to see whether we can get comparables results with fewer features.

def compute_combined_score(df, selected_features, weights):
    combined_score = pd.Series(0, index=df.index)
    # Iterate over each selected feature and its weight to compute the combined score
    for feature in selected_features:
        combined_score += weights[feature] * df[feature]
    return combined_score

# Selected features and their coefficients from the logistic regression model
"""
selected_features = {
    'Research_Approaches_Jaccard_Similarity': 2.858286,
    'Topic_Similarity_Avg': 4.381628,
    'Objectives_Max_Similarity_Max': 3.408316,
    'MeSH_Max_Similarity_Weighted_Avg': 5.412237,
    'MeSH_Avg_Similarity_Max': 2.760799
}
"""
"""
selected_features = {
	'MeSH_Max_Similarity_Weighted_Avg': 5.412237,
	'Topic_Similarity_Avg': 4.381628,
	'Objectives_Max_Similarity_Max': 3.408316,
  'Research_Approaches_Jaccard_Similarity': 2.858286,
	'MeSH_Avg_Similarity_Max': 2.760799,
	'MeSH_Avg_Similarity_Weighted_Avg': 1.917127,
	'MeSH_Max_Similarity_Avg': 1.418943,
	'Topic_Similarity_Max': 1.242131,
	'Research_Areas_Jaccard_Similarity': 1.238774,
	'Research_Type_Similarity_Score': 1.225658,
	'Methods_Max_Similarity_Max': 1.174459
}
"""

SCORES = [
    #'Assignment_Probability',
    'Combined_Score'
]

#==========================================================================================================================

#{'Research_Approaches_Jaccard_Similarity': 2.75, 'Research_Type_Similarity_Score': 0.75, 'Research_Areas_Jaccard_Similarity': 1.0,
#'Expert_Seniority': 0.5, 'Topic_PCA_1': 0.75, 'MeSH_PCA_1': 0.5}

#current_weights = {'Research_Approaches_Jaccard_Similarity': 2.75, 'Research_Type_Similarity_Score': 0.75, 'Research_Areas_Jaccard_Similarity': 1.0, 'Expert_Seniority': 0.5, 'Topic_PCA_1': 0.75, 'MeSH_PCA_1': 0.5}
current_weights = {
    'Expert_Seniority': 0.5,
    'Research_Type_Similarity_Score': 0.75,
    'Research_Approaches_Jaccard_Similarity': 2.75,
    'Research_Areas_Jaccard_Similarity': 1.0,
    'MeSH_PCA_1': 0.5,
    'Topic_PCA_1': 0.75,
    'Objectives_PCA_1': 0.25
}

# Calculate the sum of absolute values of coefficients
total_coeff_sum = sum(abs(coeff) for coeff in current_weights.values())

# Normalize to obtain weights
weights = {feature: abs(coeff) / total_coeff_sum for feature, coeff in current_weights.items()}

# Calculate combined score using weighted average of the selected features
df['Combined_Score'] = compute_combined_score(df, current_weights, weights)

# Rank experts within each project
df['Rank_Combined_Score'] = df.groupby('Project_ID')['Combined_Score'].rank(method='dense', ascending=False)

#==========================================================================================================================


PERCENTILE = 90
DISTANCE_MEDIAN = 0.10

projects_average_difficulty = {}
threshold_score = {}


for SCORE_TYPE in SCORES:
    print("="*60)
    print(f"Processing Score Type: {SCORE_TYPE}")
    print("="*60)

    # Calculate the threshold based on the percentile
    threshold_score[SCORE_TYPE] = df[SCORE_TYPE].quantile(PERCENTILE / 100)
    print(f"{SCORE_TYPE} threshold based on {PERCENTILE}% percentile: {threshold_score}\n")

    # Filter evaluators who exceed this threshold for each project
    count_above_threshold = len(df[df[SCORE_TYPE] >= threshold_score[SCORE_TYPE]])
    total_assignments = len(df)
    print(f"There are {count_above_threshold} assignments with {SCORE_TYPE} above the threshold for percentile {PERCENTILE} out of {total_assignments} total assignments.\n")

    # Initialize an empty list to store the results
    assignments_above_threshold = []

    # Iterate over each project and count the number of assignments above the fixed threshold
    project_ids = df['Project_ID'].unique()
    for project_id in project_ids:
        # Filter for the current project
        project_df = df[df['Project_ID'] == project_id]

        # Count the number of assignments with probability above the fixed threshold
        count_above_threshold = (project_df[SCORE_TYPE] >= threshold_score[SCORE_TYPE]).sum()

        # Append the results
        assignments_above_threshold.append({
            'Project_ID': project_id,
            'Assignments_Above_Threshold': count_above_threshold
        })

    # Convert the results to a dataframe for easy viewing
    assignments_above_threshold_df = pd.DataFrame(assignments_above_threshold)

    # Display the average number of assignments above threshold per project
    avg_assignments_above_threshold = assignments_above_threshold_df["Assignments_Above_Threshold"].mean()
    print(f"Average number of assignments above threshold for {SCORE_TYPE}: {avg_assignments_above_threshold}\n")

    # Calculate the median and a range for "average difficulty"
    median_assignments = assignments_above_threshold_df["Assignments_Above_Threshold"].median()
    lower_bound = median_assignments * (1-DISTANCE_MEDIAN)  # DISTANCE_MEDIAN % below the median
    upper_bound = median_assignments * (1+DISTANCE_MEDIAN)  # DISTANCE_MEDIAN % above the median

    # Identify projects with "average difficulty"
    projects_average_difficulty[SCORE_TYPE] = assignments_above_threshold_df[
        (assignments_above_threshold_df['Assignments_Above_Threshold'] >= lower_bound) &
        (assignments_above_threshold_df['Assignments_Above_Threshold'] <= upper_bound)
    ]['Project_ID'].tolist()

    print(f"Number of projects with average difficulty with {SCORE_TYPE} (within {DISTANCE_MEDIAN*100}% of median assignments): {len(projects_average_difficulty[SCORE_TYPE])}")
    print(f"Projects with average difficulty: {projects_average_difficulty[SCORE_TYPE]}\n")

#==========================================================================================================================

# Evaluation against manual assignments.

EVAL_ASSIGNMENT = 'Final_Assignment'
#EVAL_ASSIGNMENT = 'Manual_Assignment'

# Total number of manually assigned experts across all projects
total_manual_assigned_overall = df[EVAL_ASSIGNMENT].sum()

dfs_percentages = {}
best_predicted_projects = {}

print('*************** COEFFICIENTS ***************')
print(current_weights)
print()

for SCORE_TYPE in SCORES:
    print("="*60)
    print(f"Evaluating Score Type: {SCORE_TYPE}")
    print("="*60)

    # Number of top-N experts to consider:
    for NUM_TOP_EXPERTS in [1, 5, 10, 15, 20, 25]:

      # Initialize a list to store the results for each project
      percentage_in_top_n = []

      # Get the list of unique projects
      project_ids = df['Project_ID'].unique()

      # Considering "average" projects now.
      #project_ids = projects_average_difficulty[SCORE_TYPE]

      for project_id in project_ids:
          # Get the subset of data for the current project
          project_df = df[df['Project_ID'] == project_id]

          # Total number of manually assigned experts for this project
          total_manual_assigned = project_df[EVAL_ASSIGNMENT].sum()

          # Get the top-n experts based on the automatic ranking
          top_n_experts = project_df[project_df[f'Rank_{SCORE_TYPE}'] <= NUM_TOP_EXPERTS]

          # TEST !!!!!!!!!!!!!!!!
          #top_n_experts = project_df[project_df[SCORE_TYPE] >= threshold_score[SCORE_TYPE]]

          # Number of manually assigned experts in the top-n
          manual_in_top_n = top_n_experts[EVAL_ASSIGNMENT].sum()

          # Calculate the percentage
          if total_manual_assigned > 0:
              percentage = (manual_in_top_n / total_manual_assigned) * 100
          else:
              percentage = np.nan  # Handle cases where there are no manual assignments

          # Append the results for this project
          percentage_in_top_n.append({
              'Project_ID': project_id,
              'Total_Manual_Assigned': total_manual_assigned,
              'Manual_In_Top_n': manual_in_top_n,
              'Percentage_In_Top_n': percentage,
              'Top_N_Experts': len(top_n_experts)
          })

      # Convert the results to a DataFrame for easy viewing
      percentage_df = pd.DataFrame(percentage_in_top_n)
      dfs_percentages[NUM_TOP_EXPERTS] = percentage_df

      # Display the project-level results
      # print(f"\nPercentage of Manually Assigned Experts in Top-{NUM_TOP_EXPERTS} Automatic Assignments:\n")
      # display(percentage_df)

      # Calculate the overall percentage
      # Total number of manually assigned experts in the top-n across all projects
      manual_in_top_n_overall = df[df[f'Rank_{SCORE_TYPE}'] <= NUM_TOP_EXPERTS][EVAL_ASSIGNMENT].sum()

      # Calculate the overall percentage
      #overall_percentage = (manual_in_top_n_overall / total_manual_assigned_overall) * 100
      #print(f"\nOverall Percentage of Assigned Experts ({EVAL_ASSIGNMENT}) in Top-{NUM_TOP_EXPERTS}: {overall_percentage:.2f}%")

      # Number of projects with high percentage of manually assigned predicted
      #PERCENTAGE_IN_TOP = 33
      NUMBER_IN_TOP = 3
      #best_predicted_projects[NUM_TOP_EXPERTS] = percentage_df[percentage_df['Percentage_In_Top_n']>=PERCENTAGE_IN_TOP]['Project_ID'].to_list()
      best_predicted_projects[NUM_TOP_EXPERTS] = percentage_df[percentage_df['Manual_In_Top_n']>=NUMBER_IN_TOP]['Project_ID'].to_list()
      num_best_projects = len(percentage_df[percentage_df['Project_ID'].isin(best_predicted_projects[NUM_TOP_EXPERTS])])
      #print(f'Number of projects with at least {PERCENTAGE_IN_TOP}% of assigned evaluators ({EVAL_ASSIGNMENT}) predicted in top {NUM_TOP_EXPERTS}: {num_best_projects}/{len(percentage_df)}')
      print(f'Number of projects with at least {NUMBER_IN_TOP} of assigned evaluators ({EVAL_ASSIGNMENT}) predicted in top {NUM_TOP_EXPERTS}: {num_best_projects}/{len(percentage_df)}')
      #print(f'Number of projects with at least {NUMBER_IN_TOP} of assigned evaluators ({EVAL_ASSIGNMENT}) predicted above threshold {threshold_score[SCORE_TYPE]} ({percentage_df["Top_N_Experts"].mean()}): {num_best_projects}/{len(percentage_df)}')
      print(f'Percentage of projects: {(num_best_projects/len(percentage_df))*100}')
      print()

