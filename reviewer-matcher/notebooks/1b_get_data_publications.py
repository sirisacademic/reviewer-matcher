# -*- coding: utf-8 -*-
"""1b-get_data_publications.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VewM2h-d66IEXbHNHPf5Uq8pJTWTblCI

### **IMPORTS**
"""

!pip install -q python-Levenshtein

import os
import sys
import pandas as pd
import Levenshtein

from google.auth import default
from google.colab import drive
from tqdm import tqdm

"""### **SETTINGS**"""

### Test size if we want to process a subset for testing. Set as 0 to ignore.
TEST_SIZE_EXPERTS = 0

# Whether to save processed data (overriding existing).
SAVE_PUBLICATIONS = True

"""#### GOOGLE DRIVE"""

### Mount drive.
drive.mount('/content/drive')

"""####IMPORT CONFIGURATIONS AND FUNCTIONS####"""

### Add the "config" and "functions" folders to the Python path.
sys.path.append('/content/drive/MyDrive/1_Current_projects_SIRIS/2024AQUAS-ReviewerMatcher/Implementation/Notebooks/config')
sys.path.append('/content/drive/MyDrive/1_Current_projects_SIRIS/2024AQUAS-ReviewerMatcher/Implementation/Notebooks/functions')

### Import general configurations and those needed to retrieve publications.
from config_general import *
from config_get_publications import *

### Import functions to read data from spreadsheets and those needed to retrieve publications.
from functions_read_data import *
from functions_get_publications import *

print(f'Output path: {OUTPUT_PATH_PUBLICATIONS}')

### Initialization.
df_publications = pd.DataFrame()

"""### **LOAD EXPERT DATA**"""

### Load already processed experts data.
df_experts = pd.read_pickle(OUTPUT_PATH_EXPERTS)

if TEST_SIZE_EXPERTS:
  df_experts = df_experts.head(TEST_SIZE_EXPERTS)

df_experts.head(3)

"""### **GET PUBLICATION CONTENTS FROM PUBMED**"""

# Create a new dataframe to store the results
all_publications = []

# Iterate over each row in the original dataframe
for idx, row in tqdm(df_experts.iterrows(), total=df_experts.shape[0]):
    publication_titles = row['PUBLICATION_TITLES'].split('|')
    pmids_author = []
    # For each publication title, invoke the get_pubmed function.
    # Now excluding titles that do not begin with a capital letter or that are too short.
    for title in publication_titles:
        title = title.strip()
        if len(title) >= MIN_LENGTH_TITLE and title[0].isupper():
          pubmed_data = get_pubmed_content(row['FULL_NAME'], title, PUBMED_SEARCH_URL, PUBMED_PMID_URL, SEPARATOR_VALUES_OUTPUT)
          if pubmed_data:
            # Append a new row to the results list.
            distance_titles = Levenshtein.distance(title.lower(), pubmed_data['title'].lower())
            if distance_titles < THRESHOLD_DIFF_TITLES and pubmed_data['pmid'] not in pmids_author:
              pmids_author.append(pubmed_data['pmid'])
              all_publications.append({
                  'ID': row['ID'],
                  'FULL_NAME': row['FULL_NAME'],
                  'SIM_TITLES': distance_titles,
                  'TITLE_NER': title,
                  'PMID': pubmed_data['pmid'],
                  'TITLE_PUBMED': pubmed_data['title'],
                  'AUTHORS_PUBMED': pubmed_data['authors'],
                  'ABSTRACT_PUBMED': pubmed_data['abstract'],
                  'MESH_PUBMED': pubmed_data['mesh']
              })

# Create new dataframe with list of publications.
df_publications = pd.DataFrame(all_publications)

df_publications.head(10)

### Save retrieved publications.

if len(df_publications)>0 and SAVE_PUBLICATIONS:
  output_dir_publications = os.path.dirname(OUTPUT_PATH_PUBLICATIONS)
  if not os.path.exists(output_dir_publications):
    os.makedirs(output_dir_publications)
  df_publications = flatten_lists(df_publications, SEPARATOR_VALUES_OUTPUT)
  df_publications.to_pickle(OUTPUT_PATH_PUBLICATIONS)
  print(f'Retrieved publications saved to file {OUTPUT_PATH_PUBLICATIONS}')