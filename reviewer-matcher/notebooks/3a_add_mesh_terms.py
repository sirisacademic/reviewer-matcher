# -*- coding: utf-8 -*-
"""3a-add_mesh_terms.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cCiX9dvkS87mhWLgsFYBdK7YHsmfyZ13
"""

# The MeSH terms extraction model does not work with newer Transformers versions.
!pip install -q transformers==4.30

!pip install -q spacy

# TODO: Modify files before saving in the notebook that generates the data and saves the Pickle files so this is not necessary.
!pip install -q abbreviations

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !python -m spacy download en_core_web_sm

"""####**IMPORTS**"""

import pandas as pd
import torch
import spacy
import json
import abbreviations

from collections import Counter, defaultdict
from google.auth import default
from google.colab import auth
from google.colab import drive
from tqdm import tqdm
from transformers import AutoModel, AutoTokenizer, AutoModelForSequenceClassification

from google.colab import drive
drive.mount('/content/drive')

tqdm.pandas()

"""####**SETTINGS**"""

### Test size if we want to process a subset for testing. Set as 0 to ignore.

TEST_SIZE = 0

### Input/output paths.

# Specific call folder (used to retrieve the configuration, URLs, etc).
CALL_NAME = '2021-Salut Mental'

# Bath path for all the sample data.
BASE_PATH = '/content/drive/MyDrive/1_Current_projects_SIRIS/2024AQUAS-ReviewerMatcher'
#BASE_PATH = '/content/drive/MyDrive/currently/AQuAS/2024AQUAS-ReviewerMatcher'

# Code path.
CODE_PATH = f'{BASE_PATH}/Implementation/Notebooks'

# Data path.
DATA_PATH = f'{BASE_PATH}/Implementation/Data'

# Input/output file projects.
INPUT_FILE_PATH_PROJECTS = f'{DATA_PATH}/{CALL_NAME}/projects.pkl'
OUTPUT_FILE_PATH_PROJECTS = f'{DATA_PATH}/{CALL_NAME}/projects_with_mesh.pkl'

# Input/output file expert publications.
INPUT_FILE_PATH_PUBLICATIONS = f'{DATA_PATH}/{CALL_NAME}/expert_publications.pkl'
OUTPUT_FILE_PATH_PUBLICATIONS = f'{DATA_PATH}/{CALL_NAME}/expert_publications_with_mesh.pkl'

### Tasks.

# Model for labelling project sentences with rhetorical role. !! NOT USING this now - leaving as documentation!!
GET_RHETORICAL_ROLES_PROJECTS = False
# Whether to predict MeSH terms for projects.
GET_MESH_TERMS_PROJECTS = False
# Whether to predict MeSH terms for publications.
GET_MESH_TERMS_PUBLICATIONS = False

LOAD_PREVIOUSLY_PROCESSED_PROJECTS = True
LOAD_PREVIOUSLY_PROCESSED_PUBLICATIONS = True

SAVE_PROJECTS = False
SAVE_PUBLICATIONS = False

# !!!! NOT USING THIS NOW as we get objectives and methods in previous step and this classifier does not perform very well. !!!!
# Leaving as documentation.

# Sentence rhetorical role classifier.
MODEL_SENTENCE_RHETORICAL_ROLE = 'gubartz/cls_scibert_pubmed_rct'
# The model does not have a tokenizer.
TOKENIZER_SENTENCE_RHETORICAL_ROLE = 'allenai/scibert_scivocab_uncased'

# Labels returned by the model.
LABEL_OBJECTIVES = 'objective'
LABEL_METHODS = 'methods'

# New columns to add the project.
COL_PROJECT_OBJECTIVES = 'objectives'
COL_PROJECT_METHODS = 'methods'

# Model for MeSH terms extraction.

# Model.
MODEL_MESH_TERMS = 'Wellcome/WellcomeBertMesh'

EXCLUDE_MESH_TERMS = ['Animals', 'Humans', 'Female', 'Male']

THRESHOLD_MESH = 0.6

"""
# Input columns for projects and how to treat them - as strings or as lists (joined by the separator character).
INPUT_COLUMNS_PROJECTS = {
    'TITLE': 'string',
    'ABSTRACT': 'string',
    'RESEARCH_TOPIC': 'string',
    'OBJECTIVES': 'string',
    'METHODS': 'list',
    'METHODS_STANDARD': 'list',
    'METHODS_SPECIFIC': 'list'
}

# Input columns for publications.
INPUT_COLUMNS_PUBLICATIONS = {
    'TITLE_PUBMED': 'string',
    'ABSTRACT_PUBMED': 'string',
    'RESEARCH_TOPIC': 'string',
    'OBJECTIVES': 'string',
    'METHODS': 'list',
    'METHODS_STANDARD': 'list',
    'METHODS_SPECIFIC': 'list'
}
"""


INPUT_COLUMNS_PROJECTS = {
    'TITLE': 'string',
    'ABSTRACT': 'string',
    'RESEARCH_TOPIC': 'string',
    'OBJECTIVES': 'string',
    'METHODS': 'list'
}

# Input columns for publications.
INPUT_COLUMNS_PUBLICATIONS = {
    'TITLE_PUBMED': 'string',
    'ABSTRACT_PUBMED': 'string',
    'RESEARCH_TOPIC': 'string',
    'OBJECTIVES': 'string',
    'METHODS': 'list'
}

# Spacy model.
SPACY_MODEL = 'en_core_web_sm'

SEPARATOR_VALUES = '|'

"""####**FUNCTIONS**"""

# Function to classify sentences from the abstract and return those that refer to objectives and methods.
def classify_sentences(text, sent_split, tokenizer, model):
#----------------------------------------------------------
  doc = sent_split(text)
  sentences = [sent.text for sent in doc.sents]
  objective_sentences = []
  methods_sentences = []
  for sentence in sentences:
    #print(sentence)
    inputs = tokenizer(sentence, return_tensors="pt", truncation=True, padding=True)
    with torch.no_grad():
      outputs = model(**inputs)
    logits = outputs.logits
    predicted_class = torch.argmax(logits, dim=1).item()
    predicted_label = model.config.id2label[predicted_class]
    #print(f'===> {predicted_label}')
    #print('\n')
    if predicted_label == LABEL_OBJECTIVES:
      objective_sentences.append(sentence)
    elif predicted_label == LABEL_METHODS:
      methods_sentences.append(sentence)
  return objective_sentences, methods_sentences

# Function to get MeSH terms by sentences, as if we do it for the whole text we miss relevant terms.
def get_mesh_terms(text, sent_split, tokenizer, model, threshold=THRESHOLD_MESH, return_probs=False, return_occurrences=True, exclude_terms=EXCLUDE_MESH_TERMS):
#--------------------------------------------------------------------------------------------------------------------------------------------------------------
  sentences = []
  mesh_terms = []
  mesh_probs = defaultdict(float)
  if type(text) == str:
    doc = sent_split(text)
    sentences = [sent.text for sent in doc.sents]
  elif type(text) == list:
    sentences = text
  # Process each sentence.
  for sentence in sentences:
    inputs = tokenizer([sentence], padding='max_length')
    probs = model(**inputs, return_labels=False)
    if return_probs:
      for i, prob in enumerate(probs[0]):
        if prob > threshold:
          term = model.config.id2label[i]
          if term not in exclude_terms:
            mesh_probs[term] = max(mesh_probs[term], prob.item())  # Store max probability
            mesh_terms.append(term)
    else:
      labels = [model.config.id2label[i] for i, prob in enumerate(probs[0]) if prob > threshold]
      mesh_terms.extend([label for label in labels if label not in exclude_terms])
  # Return results.
  if return_probs:
    if return_occurrences:
      # Count occurrences along with max probabilities
      counts = Counter(mesh_terms)
      results = {term: {'probability': mesh_probs[term], 'count': counts[term]} for term in counts}
    else:
      # Just return terms and their maximum probabilities
      results = dict(mesh_probs)
  else:
    # Handle the case where probabilities are not required
    if return_occurrences:
      counts = Counter(mesh_terms)
      results = dict(sorted(counts.items(), key=lambda x: x[1], reverse=True))
    else:
      results = list(set(mesh_terms))
  return results

# Function to combine lists without duplicates and convert to string
def combine_mesh_lists(row, mesh_columns):
#--------------------------
  combined_set = set()
  for column in mesh_columns:
    combined_set |= set(row[column])
  return SEPARATOR_VALUES.join(combined_set)

### Load spaCy pipeline disabling not needed modules.
sent_split = spacy.load(SPACY_MODEL, disable=['tagger', 'ner', 'lemmatizer', 'textcat'])

"""####**PROJECTS - 2021**"""

### Load projects' data.

if LOAD_PREVIOUSLY_PROCESSED_PROJECTS:
  df_projects = pd.read_pickle(OUTPUT_FILE_PATH_PROJECTS).fillna('')
else:
  df_projects = pd.read_pickle(INPUT_FILE_PATH_PROJECTS).fillna('')

### Rhetorical role classifier.

# !!!! Leaving for documentation / possible future uses. !!!!
if len(df_projects)>0 and GET_RHETORICAL_ROLES_PROJECTS:
  # Load tokenizer and model.
  sent_class_tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_SENTENCE_RHETORICAL_ROLE)
  sent_class_model = AutoModelForSequenceClassification.from_pretrained(MODEL_SENTENCE_RHETORICAL_ROLE)

  # Identify 'objective' and 'methods' sentences and add as columns.
  df_projects[[COL_PROJECT_OBJECTIVES, COL_PROJECT_METHODS]] = df_projects.progress_apply(
    lambda row: pd.Series(classify_sentences(row['ABSTRACT'], sent_split, sent_class_tokenizer, sent_class_model)),
    axis=1
  )

### MeSH terms extraction model.

if GET_MESH_TERMS_PROJECTS or GET_MESH_TERMS_PUBLICATIONS:

  # Load MeSH term extraction model and tokenizer.
  mesh_terms_tokenizer = AutoTokenizer.from_pretrained(MODEL_MESH_TERMS)
  mesh_terms_model = AutoModel.from_pretrained(MODEL_MESH_TERMS, trust_remote_code=True)

if len(df_projects)>0 and GET_MESH_TERMS_PROJECTS:

  if TEST_SIZE:
    df_projects = df_projects.head(TEST_SIZE)

  # Extract MeSH terms for each column indicated in INPUT_COLUMNS_PROJECTS.
  for col_name, col_type in INPUT_COLUMNS_PROJECTS.items():
    if col_name in df_projects:
      print(col_name)
      df_projects[f'MESH_{col_name}'] = df_projects.progress_apply(
        lambda row:
          get_mesh_terms(
              row[col_name].split(SEPARATOR_VALUES) if col_type == 'list' else row[col_name],
              sent_split,
              mesh_terms_tokenizer,
              mesh_terms_model,
              return_probs=False,
              return_occurrences=False
            ),
        axis=1
      )

# Combine extracted terms into a single columns (as there is considerable overlap between them).
if len(df_projects)>0 and not 'MESH_EXTRACTED' in df_projects:
  df_projects['MESH_EXTRACTED'] = df_projects.apply(
                                    lambda row: combine_mesh_lists(row, ['MESH_TITLE', 'MESH_ABSTRACT', 'MESH_RESEARCH_TOPIC', 'MESH_OBJECTIVES', 'MESH_METHODS']),
                                    axis=1)

df_projects.head()

if len(df_projects)>0 and SAVE_PROJECTS:
  df_projects.to_pickle(OUTPUT_FILE_PATH_PROJECTS)
  print(f'Projects with MeSH terms saved to {OUTPUT_FILE_PATH_PROJECTS}')

"""### **PROJECTS - ALL**"""

# Input/output file projects.
INPUT_FILE_PATH_PROJECTS = f'{DATA_PATH}/{CALL_NAME}/all_projects.pkl'
OUTPUT_FILE_PATH_PROJECTS = f'{DATA_PATH}/{CALL_NAME}/all_projects_with_mesh.pkl'

# Whether to predict MeSH terms for projects.
GET_MESH_TERMS_PROJECTS = True

LOAD_PREVIOUSLY_PROCESSED_PROJECTS = False
SAVE_PROJECTS = True

INPUT_COLUMNS_PROJECTS = {
    'TITLE': 'string',
    'ABSTRACT': 'string'
}

### Load projects' data.

if LOAD_PREVIOUSLY_PROCESSED_PROJECTS:
  df_projects = pd.read_pickle(OUTPUT_FILE_PATH_PROJECTS).fillna('')
else:
  df_projects = pd.read_pickle(INPUT_FILE_PATH_PROJECTS).fillna('')

### Rhetorical role classifier.

# !!!! Leaving for documentation / possible future uses. !!!!
if len(df_projects)>0 and GET_RHETORICAL_ROLES_PROJECTS:
  # Load tokenizer and model.
  sent_class_tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_SENTENCE_RHETORICAL_ROLE)
  sent_class_model = AutoModelForSequenceClassification.from_pretrained(MODEL_SENTENCE_RHETORICAL_ROLE)

  # Identify 'objective' and 'methods' sentences and add as columns.
  df_projects[[COL_PROJECT_OBJECTIVES, COL_PROJECT_METHODS]] = df_projects.progress_apply(
    lambda row: pd.Series(classify_sentences(row['ABSTRACT'], sent_split, sent_class_tokenizer, sent_class_model)),
    axis=1
  )

### MeSH terms extraction model.

if GET_MESH_TERMS_PROJECTS or GET_MESH_TERMS_PUBLICATIONS:

  # Load MeSH term extraction model and tokenizer.
  mesh_terms_tokenizer = AutoTokenizer.from_pretrained(MODEL_MESH_TERMS)
  mesh_terms_model = AutoModel.from_pretrained(MODEL_MESH_TERMS, trust_remote_code=True)

if len(df_projects)>0 and GET_MESH_TERMS_PROJECTS:

  if TEST_SIZE:
    df_projects = df_projects.head(TEST_SIZE)

  # Extract MeSH terms for each column indicated in INPUT_COLUMNS_PROJECTS.
  for col_name, col_type in INPUT_COLUMNS_PROJECTS.items():
    if col_name in df_projects:
      print(col_name)
      df_projects[f'MESH_{col_name}'] = df_projects.progress_apply(
        lambda row:
          get_mesh_terms(
              row[col_name].split(SEPARATOR_VALUES) if col_type == 'list' else row[col_name],
              sent_split,
              mesh_terms_tokenizer,
              mesh_terms_model,
              return_probs=False,
              return_occurrences=False
            ),
        axis=1
      )

# Combine extracted terms into a single columns (as there is considerable overlap between them).
if len(df_projects)>0 and not 'MESH_EXTRACTED' in df_projects:
  COMBINED_COLUMNS_OUTPUT = [f'MESH_{column}' for column in INPUT_COLUMNS_PROJECTS.keys()]
  df_projects['MESH_EXTRACTED'] = df_projects.apply(
                                    lambda row: combine_mesh_lists(row, COMBINED_COLUMNS_OUTPUT),
                                    axis=1)

df_projects.head()

if len(df_projects)>0 and SAVE_PROJECTS:
  df_projects.to_pickle(OUTPUT_FILE_PATH_PROJECTS)
  print(f'Projects with MeSH terms saved to {OUTPUT_FILE_PATH_PROJECTS}')

"""####**PUBLICATIONS**"""

### Load publications' data.

if LOAD_PREVIOUSLY_PROCESSED_PUBLICATIONS:
  df_publications = pd.read_pickle(OUTPUT_FILE_PATH_PUBLICATIONS).fillna('')
else:
  df_publications = pd.read_pickle(INPUT_FILE_PATH_PUBLICATIONS).fillna('')

df_publications.head()

if len(df_publications)>0 and GET_MESH_TERMS_PUBLICATIONS:

  if TEST_SIZE:
    df_publications = df_publications.head(TEST_SIZE)

  # Extract MeSH terms for each column indicated.

  for col_name, col_type in INPUT_COLUMNS_PUBLICATIONS.items():
    if col_name in df_publications:
      print(col_name)
      df_publications[f'MESH_{col_name}'] = df_publications.progress_apply(
        lambda row:
          get_mesh_terms(
              row[col_name].split(SEPARATOR_VALUES) if col_type == 'list' else row[col_name],
              sent_split,
              mesh_terms_tokenizer,
              mesh_terms_model,
              return_probs=False,
              return_occurrences=False
            ),
        axis=1
      )

df_publications.head()

# Combine extracted terms into a single columns (as there is considerable overlap between them).
if len(df_publications)>0 and not 'MESH_EXTRACTED' in df_publications:
  df_publications['MESH_EXTRACTED'] = df_publications.apply(combine_mesh_lists, axis=1)

# Combine extracted terms into a single columns (as there is considerable overlap between them).
if len(df_publications)>0 and not 'MESH_EXTRACTED' in df_publications:
  COMBINED_COLUMNS_OUTPUT = [f'MESH_{column}' for column in INPUT_COLUMNS_PUBLICATIONS.keys()]
  df_publications['MESH_EXTRACTED'] = df_publications.apply(
                                    lambda row: combine_mesh_lists(row, COMBINED_COLUMNS_OUTPUT),
                                    axis=1)

df_publications[['TITLE_PUBMED', 'MESH_PUBMED', 'MESH_EXTRACTED']].head(10)

if len(df_publications)>0 and SAVE_PUBLICATIONS:
  df_publications.to_pickle(OUTPUT_FILE_PATH_PUBLICATIONS)
  print(f'Publications with MeSH terms saved to {OUTPUT_FILE_PATH_PUBLICATIONS}')