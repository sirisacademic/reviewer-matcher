# -*- coding: utf-8 -*-
"""5-combine_normalize_expert-project_scores.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Rjfh824Wq5252OE06CXYH_zM5q_sGALH
"""

# TODO: Modify files before saving in the notebook that generates the data and saves the Pickle files so this is not necessary.
!pip install -q abbreviations

import pandas as pd
import abbreviations
import re
import numpy as np

from google.auth import default
from google.colab import auth
from google.colab import drive
from sklearn.preprocessing import MinMaxScaler

from tqdm import tqdm

from google.colab import drive
drive.mount('/content/drive')

### Input/output paths.

# Specific call folder (used to retrieve the configuration, URLs, etc).
CALL_NAME = '2021-Salut Mental'

# Bath path for all the sample data.
BASE_PATH = '/content/drive/MyDrive/1_Current_projects_SIRIS/2024AQUAS-ReviewerMatcher'

# Code path.
CODE_PATH = f'{BASE_PATH}/Implementation/Notebooks'

# Data path.
DATA_PATH = f'{BASE_PATH}/Implementation/Data'

# Input files.
FILE_PATH_PROJECTS = f'{DATA_PATH}/{CALL_NAME}/projects.pkl'
FILE_PATH_EXPERTS = f'{DATA_PATH}/{CALL_NAME}/experts.pkl'

FILE_PATH_EXPERT_PROJECT_CONTENT_SIMILARITY_SCORES = f'{DATA_PATH}/{CALL_NAME}/scores/expert_projects_content_similarity_scores.pkl'
FILE_PATH_EXPERT_PROJECT_MESH_SCORES = f'{DATA_PATH}/{CALL_NAME}/scores/expert_projects_mesh_scores.pkl'
FILE_PATH_EXPERT_PROJECT_JACCARD_SIMILARITY = f'{DATA_PATH}/{CALL_NAME}/scores/expert_project_jaccard_similarity_scores.pkl'

# Output file
FILE_PATH_COMBINED_SCORES = f'{DATA_PATH}/{CALL_NAME}/scores/expert_projects_normalized_scores.tsv'

### Setting for matching research types.

# Define the weights for similarity scoring
WEIGHT_EXACT_MATCH = 2.0
WEIGHT_PARTIAL_MATCH = 1.0
WEIGHT_BASIC_RESEARCH_PRIORITY = 3.0
WEIGHT_RELATED_MATCH = 1.5

# Define related research type mappings
RELATED_TYPES = {
    'clinical': {'clinical', 'applied_clinical'},
    'applied_clinical': {'clinical', 'applied_clinical'},
    'basic': {'basic'},
    'epidemiology': {'epidemiology'}
}

# Research types we want to exclude from comparison
EXCLUDE_TYPES = {'translational'}

# Research type that should have high priority
PRIORITY_TYPE = 'basic'

### Setting of values for low, middle and high seniority.

SENIORITY_UNDETERMINED = 0
SENIORITY_LOW = 1
SENIORITY_MIDDLE = 2
SENIORITY_HIGH = 3

### Settings

SEPARATOR_VALUES = '|'

NUM_PUBS_TOP_PERC_SENIORITY = 30
NUM_CITATIONS_TOP_PERC_SENIORITY = 30

### Test size if we want to process a subset for testing. Set as 0 to ignore.

TEST_SIZE_PROJECTS = 0
TEST_SIZE_EXPERTS = 0

# Load data.

df_projects = pd.read_pickle(FILE_PATH_PROJECTS).fillna('')
df_experts = pd.read_pickle(FILE_PATH_EXPERTS).fillna('')

if TEST_SIZE_PROJECTS:
  df_projects = df_projects.head(TEST_SIZE_PROJECTS)

if TEST_SIZE_EXPERTS:
  df_experts = df_experts.head(TEST_SIZE_EXPERTS)

### Functions

# Function to extract the first valid numeric value from a text
def extract_number(text):
#--------------------------------
    if type(text) == int:
      return text
    # Check for empty or NaN values
    if pd.isna(text) or text.strip() == '':
        return 0
    # Check if the entire text is a URL, if so return 0
    if re.match(r'^(https?://)', text.strip()):
        return 0
    # Extract all numbers, including those with commas
    numbers = re.findall(r'\b\d{1,10}(?:,\d{3})*|\d+\b', text)
    if numbers:
        # Clean commas and convert all numbers to integers
        cleaned_numbers = [int(num.replace(',', '')) for num in numbers]
        # Return the largest number found
        return max(cleaned_numbers)
    return 0

# Function to determine seniority level: low, middle, high, or undetermined
def determine_seniority(row, pub_threshold, citations_threshold):
#--------------------------------
    # Determine seniority based on publication, citation, and experience, giving more relevance to EXPERIENCE_REVIEWER and NUMBER_PUBLICATIONS
    if row['NUMBER_PUBLICATIONS'] == 0 and row['NUMBER_CITATIONS'] == 0:
      if row['EXPERIENCE_REVIEWER'] == 'yes' or row['EXPERIENCE_PANEL'] == 'yes':
        return SENIORITY_UNDETERMINED
      else:
        return SENIORITY_LOW
    # High seniority if NUMBER_PUBLICATIONS and EXPERIENCE_REVIEWER are strong, even if other factors are moderate
    elif row['NUMBER_PUBLICATIONS'] >= pub_threshold and row['EXPERIENCE_REVIEWER'] == 'yes':
        return SENIORITY_HIGH
    # Middle seniority if NUMBER_PUBLICATIONS or EXPERIENCE_REVIEWER are strong, but not both, or other experience factors are met
    elif row['NUMBER_PUBLICATIONS'] >= pub_threshold or row['EXPERIENCE_REVIEWER'] == 'yes':
        if row['EXPERIENCE_PANEL'] == 'yes' or row['NUMBER_CITATIONS'] >= citations_threshold:
            return SENIORITY_MIDDLE
        else:
            return SENIORITY_LOW
    # Middle seniority if both experience factors are positive, even if publications/citations are not
    elif row['EXPERIENCE_REVIEWER'] == 'yes' and row['EXPERIENCE_PANEL'] == 'yes':
        return SENIORITY_MIDDLE
    # Low seniority if none of the above apply but some experience is present
    else:
        return SENIORITY_LOW

# Function to compute the similarity score for research types
def compute_research_type_similarity(expert_types, project_types):
#----------------------------------------------------------------
    # Convert types to sets and filter out types we want to exclude
    expert_set = set(expert_types.split('|')) - EXCLUDE_TYPES
    project_set = set(project_types.split('|')) - EXCLUDE_TYPES
    # Compute overlap
    common_types = expert_set.intersection(project_set)
    # Start with a basic score based on the number of common research types
    score = len(common_types) * WEIGHT_PARTIAL_MATCH
    # Add weight if there is an exact match
    if expert_set == project_set:
        score += WEIGHT_EXACT_MATCH
    # Boost the score significantly if both contain the priority type
    if PRIORITY_TYPE in expert_set and PRIORITY_TYPE in project_set:
        score += WEIGHT_BASIC_RESEARCH_PRIORITY
    # Check for related matches
    for project_type in project_set:
        if project_type in RELATED_TYPES:
            related_types = RELATED_TYPES[project_type]
            related_match_count = len(expert_set.intersection(related_types))
            score += related_match_count * WEIGHT_RELATED_MATCH
    return score

# Convert Expert_Number_Publications and Expert_Number_Citations columns to numbers
df_experts['NUMBER_PUBLICATIONS'] = df_experts['NUMBER_PUBLICATIONS'].apply(extract_number)
df_experts['NUMBER_CITATIONS'] = df_experts['NUMBER_CITATIONS'].apply(extract_number)

# Determine the threshold for "high" publications and citations
pub_threshold = np.percentile(df_experts['NUMBER_PUBLICATIONS'], NUM_PUBS_TOP_PERC_SENIORITY)
citations_threshold = np.percentile(df_experts['NUMBER_CITATIONS'], NUM_CITATIONS_TOP_PERC_SENIORITY)

print(f'pub_threshold: {pub_threshold}')
print(f'citations_threshold: {citations_threshold}')

# Compute seniority - numerical and categorical.
df_experts['SENIORITY'] = df_experts.apply(lambda row: determine_seniority(row, pub_threshold, citations_threshold), axis=1)

# Load expert-project similarity scores.

df_content_similarity_scores = pd.read_pickle(FILE_PATH_EXPERT_PROJECT_CONTENT_SIMILARITY_SCORES)
df_mesh_scores = pd.read_pickle(FILE_PATH_EXPERT_PROJECT_MESH_SCORES)
df_jaccard_similarity_scores = pd.read_pickle(FILE_PATH_EXPERT_PROJECT_JACCARD_SIMILARITY)

# Ensure uniformity of columns in all dataframes.
columns_projects = {
    'ID': 'Project_ID',
    'TITLE': 'Project_Title',
    'RESEARCH_TYPE': 'Project_Research_Types'
}

### !!! TODO - Handle cases where some columns do not exist - for instance GENDER or NUMBER_CITATIONS/NUMBER_PUBLICATIONS !!! (see calls 2015/2016)

columns_experts = {
    'ID': 'Expert_ID',
    'FULL_NAME': 'Expert_Full_Name',
    'GENDER': 'Expert_Gender',
    'RESEARCH_TYPES': 'Expert_Research_Types',
    'SENIORITY': 'Expert_Seniority',
    'EXPERIENCE_REVIEWER': 'Expert_Experience_Reviewer',
    'EXPERIENCE_PANEL': 'Expert_Experience_Panel',
    'NUMBER_PUBLICATIONS': 'Expert_Number_Publications',
    'NUMBER_CITATIONS': 'Expert_Number_Citations'
}

# Include columns from df_projects
df_projects = df_projects[list(columns_projects.keys())].rename(columns=columns_projects)

# Include columns from df_experts
df_experts = df_experts[list(columns_experts.keys())].rename(columns=columns_experts)

# Make sure we use consistent types.
df_projects['Project_ID'] = df_projects['Project_ID'].astype(str)
df_experts['Expert_ID'] = df_experts['Expert_ID'].astype(str)
df_experts['Expert_Full_Name'] = df_experts['Expert_Full_Name'].str.strip()

for df in [df_content_similarity_scores, df_mesh_scores, df_jaccard_similarity_scores]:
  df['Project_ID'] = df['Project_ID'].astype(str)
  df['Expert_ID'] = df['Expert_ID'].astype(str)

df_jaccard_similarity_scores.columns

df_mesh_scores.columns

df_content_similarity_scores.columns

### Combine data.

# Merge Jaccard similarity, MeSH scores, and content similarity into a single dataframe
df_combined_similarity_scores = pd.merge(df_jaccard_similarity_scores, df_mesh_scores, on=['Expert_ID', 'Project_ID'], how='inner')
df_combined_similarity_scores = pd.merge(df_combined_similarity_scores, df_content_similarity_scores, on=['Expert_ID', 'Project_ID'], how='inner')

# Loop through columns in the DataFrame and rename them if they have the prefix "Expert_"
new_column_names = {}
for col in df_combined_similarity_scores.columns:
    if col.startswith('Expert_') and col != 'Expert_ID':
        new_column_names[col] = col.replace('Expert_', '', 1)

# Add projects / experts columns to the DataFrame and rename.
df_combined_similarity_scores.rename(columns=new_column_names, inplace=True)
df_combined_similarity_scores = pd.merge(df_combined_similarity_scores, df_projects, on='Project_ID', how='left')
df_combined_similarity_scores = pd.merge(df_combined_similarity_scores, df_experts, on='Expert_ID', how='left')

# Calculate similarity scores for each expert-project pair
df_combined_similarity_scores['Research_Type_Similarity_Score'] = df_combined_similarity_scores.apply(
    lambda row: compute_research_type_similarity(row['Expert_Research_Types'], row['Project_Research_Types']),
    axis=1
)

# Normalize columns.

# Initialize the scaler
scaler = MinMaxScaler()

columns_to_normalize = ['Research_Type_Similarity_Score', 'Expert_Seniority']

# Fit and transform the scores
df_combined_similarity_scores[columns_to_normalize] = scaler.fit_transform(df_combined_similarity_scores[columns_to_normalize])

# Reorder the DataFrame using the defined column order

# Define the desired column order
column_order = [
    # Expert-specific columns
    'Expert_ID', 'Expert_Full_Name', 'Expert_Gender', 'Expert_Research_Types', 'Expert_Seniority',
    'Expert_Experience_Reviewer', 'Expert_Experience_Panel', 'Expert_Number_Publications', 'Expert_Number_Citations',
    # Project-specific columns
    'Project_ID', 'Project_Title', 'Project_Research_Types',
    # Similarity scores (Jaccard, Content, MeSH)
    'Research_Type_Similarity_Score',
    'Research_Areas_Jaccard_Similarity', 'Research_Approaches_Jaccard_Similarity',
    'Topic_Similarity_Max', 'Topic_Similarity_Avg',
    'Objectives_Max_Similarity_Max', 'Objectives_Max_Similarity_Avg',
    'Objectives_Avg_Similarity_Max', 'Objectives_Avg_Similarity_Avg',
    'Methods_Max_Similarity_Max', 'Methods_Max_Similarity_Avg',
    'Methods_Avg_Similarity_Max', 'Methods_Avg_Similarity_Avg',
    'Methods_Max_Similarity_Weighted_Max', 'Methods_Max_Similarity_Weighted_Avg',
    'Methods_Avg_Similarity_Weighted_Max', 'Methods_Avg_Similarity_Weighted_Avg',
    'MeSH_Semantic_Coverage_Score', 'MeSH_Max_Similarity_Max',
    'MeSH_Max_Similarity_Avg', 'MeSH_Avg_Similarity_Max',
    'MeSH_Avg_Similarity_Avg', 'MeSH_Max_Similarity_Weighted_Max',
    'MeSH_Max_Similarity_Weighted_Avg', 'MeSH_Avg_Similarity_Weighted_Max',
    'MeSH_Avg_Similarity_Weighted_Avg'
]

df_combined_similarity_scores = df_combined_similarity_scores[column_order]

# Save to file.
df_combined_similarity_scores.to_csv(FILE_PATH_COMBINED_SCORES, sep='\t', index=False)

df_combined_similarity_scores.columns